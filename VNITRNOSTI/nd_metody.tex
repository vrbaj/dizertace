\chapter{Přehled metod detekce novosti}
Tato kapitola je věnována přehledu různých vybraných přístupů k detekci novosti, které se v posledních letech používají. Více pozornosti je věnováno algoritmům Learning Entropy (viz kap.\ref{chap:LE}) a Error and Learning Based Novelty detection (viz kap. \ref{chap:elbnd}) které jsou v kapitole (XYZ) použity pro porovnání úspěšnosti detekce novosti.
\par
Vzhledem k dlouhé historii oblasti detekce novosti vznikla celá řada přehledových publikací, které metody detekce novosti rozdělují do různých skupin. Miljkovič v \cite{miljkovic} rozděluje metody detekce novosti na metody založené na klasifikaci (pravidlové systémy, neuronové sítě a SVM), nejbližší sousedy (vzdálenost, hustota), klastrovací metody, statistické (parametrické a neparametrické) a ostatní (teorie informace, spektrální dekompozice, vizualizace).
\par 
Zimek a Filzmozer \cite{zimek} rozdělují metody ND opět na celou řadu kategorií. Na metody s učitelem a bez učitele, na metody založené na skóre nebo příslušnosti ke množině přičemž metody založené na skóre přiřazují datům nějaké číselné ohodnocení podle kterého se rozhoduje, zda jde o data nové či nikoliv. Metody založené na příslušnosti zařazují data rovnou do jedné ze dvou množin označené "normální" a "outlier". Další rozdělení je na metody parametrické a neparametrické. Mezi parametrické metody autoři řadí metody založené na konkrétních pravděpodobnostních rozložení (např. gaussovké). Neparametrické metody jsou metody, které nepracují s parametry pravděpodobnostních rozdělení. 
\par 
Jednu z nejobsáhlejších přehledových publikací napsali Markou a Singh \cite{markou1,markou2}. První díl jejich přehledu je věnován statistickým metodám ND a druhý díl je věnován metodám založených na neuronových sítích. V první části zabývající se statistickými metodami autoři přichazejí s dělením na metody parametrické (pravděpodobností, gaussian mixture modely, skryté markovské modely a testování hypotéz), a neparametrické (kNN, Parzenovo okno, porovnávání řetězců a klastrovací metody). Metody založené na neuronových sítích pak dělí ne přístupy využívající vícevrstvý perceptron, metodu podpůrných vektorů, teorii adaptivní rezonance, sítě typu RBF (radial basis function), autoasociátory, Hopfieldovy sítě, oscilující sítě, samoorganizujícíc se mapy, neuronové stromy, sítě s neasociativním učením a ostatní přístupy.
\par
Nejnovější extenzivní publikací je velice rozsáhlá přehledová publikace čítající přes 300 referencí byla vytvořena Pimentelem, D. a L. Cliftonovými a Tarassenkem \cite{pimentel}. Autoři zde dělí metody ND na pravděpodobnostní (parametrické a neparametrické), metody založené na vzdálenosti (nejbližší soused, klastrovací metody), metody založené na rekonstrukci (neuronové sítě, metody založené na podprostorech), metody vycházející z teorie informace a doménové metody (SVM).
\section{Vybrané statistické metody}

\subsection{Metody založené na teorii extrémních hodnot}
Teorie extrémních hodnot (EVT) se zabývá distribucí dat s abnormálně velkými nebo malými hodnotami z ocasů generujících rozdělení \cite{evt1}. Z hlediska teorie extrémních hodnot jsou klíčové dvě věty. Fisher-Tippett-Gnedenkova věta, podle které rozdělení maximum vzorků nezávislých náhodných proměnných se stejným rozdělením po vhodné normalizaci konverguje k jednomu ze tří rozdělení extrémních hodnot (Fréchetovo, Gumbelovo nebo Weibullovo) \cite{evt2}. Druhou větou je Pickand-Balkema-de Haanova věta \cite{gpd5,gpd6}, která je uvedena v kapitole \ref{chap:gpd}, a která ukazuje důležitost zobecněného Paretova rozdělení pro modelování ocasů různých typů rozdělení.
\par 
V \cite{evt1} autoři porovnávají výsledky na datech pacientů s tremorem získané pomocí EVT a GMM a ukazují, že při použití EVT získají méně falešně pozitivních výsledků. Pro rozlišení zda má pacient tremor používají hodnotu prahu $P=0.95$. V \cite{evt3} autoři přicházejí se zobecněním generalizovaného rozdělení extrémních hodnot pro práci s vícerozměrným a vícemodálním rozdělením. Dále navrhují nové číselné ohodnocení míry novosti, které lze interpretovat jako pravděpodobnost, že extrémní hodnota bude blíž středu rozdělení (ve smyslu Mahalanobisovy vzdálenosti \cite{maha}). Použitelnost nového přístupu pak demonstrují na datech pacientů u nichž je nepřetržitě monitorován srdeční tep a rychlost dýchání. Při zachování úspěšnosti detekce skutečně pozitivních pacientů (ve smyslu krizové situace) snižují o $58\%$ falešně pozitivní detekce. V další obdobné publikaci \cite{evt5} pak autoři přicházejí s použitím zobecněného Paretova rozdělení  ve vícerozměrných problémech. EVT nalézá své uplatnění i v kombinaci s Kalmanovým filtrem \cite{kalman}. V \cite{evt4} autoři detekují novost v datech pomocí AR modelu Kalmanova filtru. Pomocí EVT modelují pravděpodobnost hodnoty koeficientů AR modelu filtru a přičemž pro rozlišení novosti používají velikost prahu $P=0.95$. Použitelnost uvedeného přístupu demonstrují na umělých datech, ale např. i na datesetu obsahující data indexu Dow Jones v období 1972-1975 (pro zajímavost uveďme, že se autorům podařilo detekovat odsouzení spolupracovníků prezidenta Nixona v aféře Watergate, uvalení embarga OPEC na státy podporující Izrael během Jomkipurské války a rezignaci prezidenta Nixona). V \cite{evt6} autoři navrhují novou metodu  adaptivní volby prahu pro zobecněné Paretovo rozdělení a ukazují možnost využití v oblasti kybernetické bezpečnosti, kde úspěšně detekují pokus o penetraci sítě. Navrhovaná metoda využívá počtu naměřených dat, požadovaného hodnoty pravděpodobnosti, počtu překročení stávajícího prahu a k němu příslušných hodnot parametrů GPD. Publikace \cite{evt7} je věnována odhadu parametrů GPD pomocí metody maximální věrohodnosti kombinované s optimalizací hejnem částic (PSO - Particle Swarm Optimization \cite{pso}). Autoři ukazují, že jimi navrhovaný přístup přináší výrazné zlepšení v přesnosti odhadu parametrů. Problematice detekce novosti v point pattern datech (data která jsou tvořena množinami vektorů) se věnují autoři v \cite{evt8}. Problémem této oblasti je obrovský počet dimenzí a chybějící data v oblastech extrémů, z čehož plyne nutnost pracovat s řídkými daty. Proto autoři navrhují algoritmus pro extrapolaci modelu abnormálních dat z modelu normálních dat a zakomponování dostupných informacích o extrémech z dostupných dat. 
\subsection{Metody využívající Gaussovy smíšení modely}
Gaussovské smíšené modely (GMM - Gaussian Mixture Models) jsou modely, které se používají pro aproximaci rozdělení pomocí lineární kombinace normálních  rozdělení \cite{gmm1}. Podle \cite{gmm2} lze téměř jakoukoliv spojitou funkci hustoty pravděpodobnosti aproximovat s libovolnou přesností pomocí dostatečného počtu gaussovských komponent s vhodně zvolenými středními hodnotami, kovariencemi a váhovacími koeficienty (více např. v \cite{gmm3,gmm4}). Tyto modely jsou vhodné i pro vícerozměrná data, je-li k dispozici dostatečně velké množství dat pro odhad jejich parametrů. Rozložení pravděpodobnosti klasického GMM je definováno jako
\begin{equation}
f(\textbf{x})=\sum_{i=1}^M K_i\mathcal{N}_i(\textbf{x})
\end{equation}
přičemž určité úskalí skýtá potřeba apriorní informace o vektoru středních hodnot, kovarianční matici a příslušném váhovacím koeficientu $K_i$ dané $i$-té gaussovské komponenty. 
\par V oblasti ND se pak obvykle určí hodnota hustoty pravděpodobnosti GMM daného vzorku dat a podle definovaného prahu se určí zda se jedná o data nová či nikoliv.  V \cite{gmm2} používají autoři GMM k monitoringu převodovky. Zpracovávaným signálem jsou naměřené vibrace převodovky. Tento signál je segmentován a na základě segmentů signálů bez poruchy jsou odhadnuty parametry GMM. Při monitoringu je potom pro každý segment je spočítána hodnota negativní logaritmické věrohodnostní funkce (NLL) \cite{gmm5}. Autoři ukazují, že vysoké hodnoty NLL korespondují s poškozením konkrétních zubů převodovky. V \cite{gmm6} používají autoři VB-GMM (Variational Bayesian Gaussian Mixture Model) k detekci poruch plynových turbín. Oproti klasickému GMM, kde je počet gaussovských komponent zvolen apriori, využívá VB-GMM variační Bayesovské metody k určení optimálního množství komponent \cite{gmm7}. Autoři ukazují, že pomocí zvolené metody jsou s vysokou mírou spolehlivosti rozpoznat různé závady, které korespondují s novostí v měřených datech. Uplatnění GMM je i v oblasti finančnictví, konkrétně v detekci insider tradingu. V příspěvku \cite{gmm8} používají autoři standartní GMM pro modelování poptávky na Tokijské burze a na odhalených případech insider tradingu demonstrujou výhody publikovaného přístupu. Úskalím používání složitých GMM je velký počet jejich parametrů. Čím více parametrů zvolený model má, tím více dat je potřeba k jejich odhadu a ověření daného modelu. Pro nalezení parametrů GMM je často používána EM metoda (Expectation-maximization) \cite{gmm9}, případně její výpočetně méně náročná modifikace \cite{gmm10}. Další aplikace GMM a smíšených metod lze nalézt v aktuální přehledové publikaci \cite{gmm11}.

\subsection{Markovovy skryté modely Nebo entropie?}
\# TODO

\section{Neuronové sítě}
V této podkapitole je uveden menší souhrn přístupů  využívajících různé typy neuronových sítí. Jmenovitě se jedná metodu podpůrných vektorů, samoorganizující se mapy, konvoluční neuronové sítě a autoenkodéry, které se v posledních letech začali v oblasti ND uplatňovat velice často.
\subsection{Metoda podpůrných vektorů}
Metoda podpůrných vektorů (SVM - support vector machine) bývá některými autory řazena mezi neuronové sítě (např. \cite{markou2}). Vzhledem k tomu, že lineární verze SVM \cite{svm1} je podobně jako perceptron \cite{perceptron} lineární klasifikátor a spadá také do oblasti strojového učení \cite{ml1,ml2}, zařadili jsme ji do kapitoly věnované neuronovým sítím.
\par 
Obecně SVM funguje tak, že rozdělí trénovací data v prostoru na dvě oblasti. Data v každé oblasti náleží do příslušné třídy a jsou oddělena rozhodovací hranicí. Rozhodovací hranice odděluje data tak, že vzdálenost dat z obou různých tříd je od této hranice maximální \cite{svm3}. Lineární SVM rozdělí data pomocí nadroviny, pokud jsou data lineárně separabilní. Problém nastává v okamžiku, kdy data nejsou plně lineárně separabilní, což může být např.  případě dat obsahujících šum.
\par 
Nelineární SVM \cite{svm1} zobrazí data do prostoru o vyšší dimenzi pomocí vhodného nelineárního zobrazení a následně je nalezena nadrovina, která data rozdělí. Zobrazením dat do prostoru o vyšší dimenzi se tedy převede problém na lineárně separabilní. Zobrazení dat do prostoru o vyšší dimenzi se realizuje jádrovou transformací, přičemž jako jádrové funkce se obvykle používá funkce polynomiální, RBF, sigmoida, dvouvrstvý perceptron a podobné. Existují ale i jádrové funkce pracující s řetězci. Podrobnosti o požadavcích pro jádrové funkce jsou uvedeny např. v \cite{svm4}. Pro snížení výpočetního času, který s přechodem do vyšších dimenzí roste, se využívá tzv. jádrového triku (kernel trick) \cite{kernel}. I to jsou důvody, proč je SVM velice efektivní a populární metodou v oblasti detekce novosti.
\par 
Z pohledu oblasti detekce novosti je klíčová publikace \cite{svm5}, kde autoři využívají SVM následujícím způsobem. Pro trénovací data se nalezne rozhodovací křivka taková, že předem stanovená část trénovacích dat se nalézá v oblasti vymezené rozhodovací hranicí. Detekce novosti je pak realizována tak, veškerá data, která leží vně jsou klasifikována jako nová. Ve studii věnované online detekce poruch chladících jednotek \cite{svm6} autoři používají rozšířený Kalmanův filtr pro odhad stavu jednotky v kombinaci s rekurzivní jednotřídní SVM. Na velkém vzorku dat bez poruchy natrénují SVM, přičemž data pro trénování jsou odhadnuté parametry modelu jednotky. Při monitorování potom v konstantních časových periodách rozšiřují trénovací množinu a přetrénují SVM (princip rekurzivní SVM), ovšem pouze v případě, že se v datech ještě neprojevila porucha. Porucha chladící jednotky se projeví změnou parametrů modelu rozšířeného Kalmanova filtru, což je detekováno pomocí SVM. Použitím rozšířeného Kalmanova filtru redukují oproti stávajícím metodám počet potřebných atributů k vyhodnocování a snižují počet potřebných senzorů k monitoringu. Uvedeným postupem dosahují také vyšší úspěšnost detekce poruchy oproti stávajícím metodám. Autoři uvádějí, že jejich úspěšnost detekce je až $95\%$. V \cite{svm7} rozšiřují klasický jednotřídní SVM přístup k detekci novosti o nelineární atributy. Pro naměřená data určí autoři jejich maximum, minimum, střední hodnotu, hodnotu špička - špička, medián, směrodatnou odchylku, kvadratický průměr, šikmost, špičatost a činitel výkonu. K těmto charakteristikám naměřených dat autoři přidávájí další nelineární atributy, konkrétně approximate entropy \cite{ae}, sample entropy \cite{se} a permutační entropii \cite{pe}. Navrhovaným přístupem autoři dosahují lepších výsledků při detekci poruchy ložiska oproti stávajícím používaným metodám detekce. V práci \cite{svm8} používají autoři SVM k detekci nových objektů na snímcích infračerveného kosmického teleskopu (WISE). Autoři testují různé typy jádrových funkcí (lineární, sigmoidní, rbf a polynomiální) na velkém datasetu čítajícím 747 milionů oklasifikovaných objektů a ukazují, že pro hledání nových objektů je nejvhodnější RBF jádrová funkce. Dále uvádějí, že  jimi dosažené výsledky jsou lepší, co se týče počtu detekcí neznámých objektů, oproti předchozím publikovaným metodám. 
\subsection{Samoogranizační mapy}
Samoorganizační mapy (SOM) \cite{som0} jsou umělé neuronové sítě, které více rozměrná vstupní data reprezentují pomocí 1-D nebo 2-D mapy, v podstatě provádějí vektorovou kvantizaci vstupního prostoru. Mapa je tvořena neurony, jejichž vzájemná pozice by měla odpovídat topologii vstupních dat v tom smyslu, že vstupní vektory, které byly podobné (ve smyslu jejich vzdálenosti) by měli být zobrazeny na neurony, které spolu v mapě sousedí \cite{somS2}. Tyto sítě se učí pomocí kompetitivního učení a v praxi se často používají pro redukci dimenze zpracovávaných dat \cite{somS}. SOM ale nalézají i uplatnění v detekci novosti, o čemž se lze přesvědčit např. v extenzivní přehledové publikaci \cite{somS2}. 
\subsection{Konvoluční neuronové sítě}
S rozvojem hlubokého učení se dostali v poslední době do popředí i konvoluční neuronové sítě (CNN). Zásadní publikací z pohledu teorie CNN je potom  \cite{cnn1}. Nutno poznamenat, že CNN nenašli své uplatnění jen ve zpracování obrazu, ale také ve zpracování časových řad. V následujícím textu považujeme za CNN jakoukoli neuronovou síť, která má alespoň vrstvu používající konvoluci (což je ve shodě s \cite{deep}). Předností CNN je jejich vysoká úspěšnost detekce, respektive klasifikace. Ta je všem dosažena pomocí velkého množství parametrů sítě, z čehož plyne i požadavek na dostatečně velký vzorek dat pro natrénování. Samostatným problémem je pak volba architektury CNN ve smyslu počtu vrstev, velikosti konvolučních jader,  volbě pooling vrstvy, učícím algoritmu apod.\par
S velice zajímavou aplikaci CNN pro detekci novosti přicházejí autoři v \cite{cnn2}. Pro monitoring technického stavu staveb využívají data naměřená akcelerometrem. Naměřená data segmentují, a tyto segmenty převedou v časové i frekvenční oblasti převedou na obrazy, které zobrazují průběh amplitudy v čase, respektive výkon pro různé frekvence. Tato reprezentace dat je obvyklá posouzení expertem. Výše uvedeným způsobem předzpracovaná data pak slouží k natrénování CNN. Pro detekci anomálií, které souvisí s potenciální závadou na stavbě pak používají stejným způsobem předpřiravená data, která jsou kontinuálně měřena. Konkrétně jde o data z několika akcelerometrů, které sledují stav mostu. Autoři dále uvádějí, že uvedeným přístupem lze zpracovávat libovolné časové řady. V publikaci \cite{cnn3} autoři používají CNN k detekci defektů na snímcích povrchů. Pro natrénovaní CNN používají data které neobsahují žádné defekty. Vstupem do neuronové sítě jsou tři obrazy. Prvním a druhým obrazem jsou různé snímky stejného povrchu bez defektu a třetím je obraz jiného povrchu. Trénování sítě pak minimalizuje vzdálenost mezi prvními dvěma obrazy (vstup a prototyp) a maximalizuje vzdálenost mezi prvním a třetím. Při testování je výstupem sítě obraz, jehož intenzita znázorňuje vzdálenost oblasti obrazu od prototypu. Oblasti s vysokou vzdáleností jsou pak oblasti s defektem.  Ve studii zabývající se detekcí patologie srdce s využitím fonokardiogramu \cite{cnn4} autoři používají CNN k detekci abnormálních zvuků srdce. Pro natrénování CNN fonokardiogram autoři nasegmentují a pro každý segment vytvoří mel kepstrum, obálku a mel spektrogram.  Výsledkem je klasifikátor s přesností $0.815$, sensitivitou $0.845$ a specifitou $0.785$ což jsou výsledky, kterých dosahují nejlepší automatizované klasifikační systémy. Kombinaci autoenkodéru a CNN používají autoři pro detekci nových objektů v obrazech v publikaci \cite{cnn5}. CNN slouží pouze pro rozlišení mezi známou a neznámou třídou. Autoenkodér slouží pro posílení klasifikační schopnosti CNN v tom smyslu, že během předzpracování obrazu pro CNN odstraní šum z obrazů známých tříd a zkreslí obrazy neznámých tříd. Tím je dosažena zvýšená přesnost detekce neznámé třídy pomocí CNN. V \cite{cnn6} používají autoři CNN k detekci abnormálního chování  z pohledu denních aktivit. Další oblast využití je detekce novosti předložených dokumentů \cite{cnn7}, přičemž využití je např. v systémech kontroly plagiátorství. V \cite{cnn8} autoři přidávají do latentní oblasti prostoru atributů (což je defakto výstup konvoluční vrstvy) gaussovský šum a tím zvyšují úspěšnost detekce novosti. V \cite{cnn9} pak autoři navrhují metodiku pro zjištění, která část CNN je zásadní pro rozlišení neznámé třídy.
\subsection{Autoenkodéry}
Autoenkodéry jsou speciálním typem neuronových sítí. Jejich specifikem je, že vstupní i výstupní vrstva obsahují stejný počet neuronů. Počty neuronů ve skrytých vrstvách jsou však menší. Při učení těchto sítí je cílem získat na výstupu enkodéru identický vektor dat jako je na vstupu. Každý autoenkodér můžeme reprezentovat pomocí dvou sítí. Kódovací síť realizuje zobrazení z prostoru vzorů do prostoru příznaků. Vzhledem k rozdílné dimenzi těchto prostorů je možné uvedenou operaci označit jako kódování. Uvažujme vstupní vektor $x \in R^n$. Reprezentace tohoto vektoru ve skryté vrstvě $z(x)\in R^m$ je obecně určená funkcí 
\begin{equation}
z(x)=f_1(W_1 x+b_1) 
\end{equation}
kde $f_1$ je nelineární aktivační funkce, $W_1 \in R^{n\times m}$ je matice vah a $b_1 \in R^m$ je vektor příslušných biasů. Tato zakódovaná reprezentace vzoru $x$ je zobrazena do prostoru rekonstruovaných vzorů $\hat{x}\in R^n$, přičemž toto zobrazení je realizováno výstupní vrstvou autoenkodéru, tedy
\begin{equation}
\hat{x}=f_2(W_2z(x)+b_2)
\end{equation}
kde $f_2$ je vhodná funkce, $W_2\in R^{m \times n}$ je matice vah a $b_2$ je vektor příslušných biasů \cite{auto1}. Při učení autoenkodérů se obvykle minimalizuje tzv. chyba rekonstrukce, definovaná jako
\begin{equation}
\mathcal{L}(x,\hat{x})= \lvert\lvert x - \hat{x}\rvert \rvert^2
\end{equation}
pomocí metody zpětného šíření (backpropagation).
\par 
V \cite{auto2} používají autoři autoenkodér pro detekci novosti tak, že nejprve autoenkodér naučí a poté mu předkládají různé vzory. Podle chyby rekonstrukce pak usuzují, zda lze předložený vzor klasifikovat jako nový nebo nikoliv. Rozhodovacím kritériem je tedy velikost chyby rekonstrukce $\mathcal{L}$. Pokud je chyba rekonstrukce větší než zvolená prahová hodnota, pak je vzor klasifikován jako nový. Stejné kritérium pro detekci novosti v audio datech používají i autoři v \cite{auto3}. Požívají ale modifikovanou strukturu autoenkodéru, jehož skrytá vrstva obsahuje více neuronů než vstupní a výstupní vrstva (denoising autoecoder). Tento typ autoenkodérů bývá využit k odstranění šumu v časových řadách.
V \cite {auto1} navrhují autoři pro detekci novosti využít odhad hustoty skryté vrstvy. Pokud vstup autoenkodéru má nízkou hustotu ve skryté vrstvě, je tento vstup klasifikován jako nový. Pro modelování hustoty skryté vrstvy autoři navrhují používat jádrový odhad. Oblastí, ve které nacházejí autoenkodéry uplatnění je i oblast hlubokého učení. V \cite{auto4} je použito série autoenkodérů, kde vstupem do každého enkodéru je výstup předcházejícího. Je tedy nutné pracovat se řadou chyb rekonstrukce. Práh pro určení zda-li se jedná o data nová, označme $\beta$, pak autoři navrhují ve tvaru
\begin{equation}
\beta = \alpha \times median(\mathcal{L}_1,\dots \mathcal{L}_n)
\end{equation}
kde $n$ je počet použitích autoenkodérů a $\alpha$ je volitelný koeficient.  Tento práh je porovnáván s celkovou chybou rekonstrukce sítě autoenkodérů.
Ve článku \cite{auto5} je navržen nový typ autoenkodéru DGP-EV (Deep Gaussian Process autoencoder), jehož kódovací a dekódovací část využívá gaussovských procesů. Předností uvedené sítě je možnost využití v oblastech, kde se pracuje s daty kategorickými i numerickými. Autoenkodéry nalézají uplatnění i v oblastech zpracování obrazu. Pro kategorizaci objektů v obraze a detekci neznámých objektů používají autoři v \cite{auto6} autoenkodér s dvěmi dekodovacími sítěmi, pro jehož učení využívají algoritmus GZSL (Generalized Zero Shot Learning). Pro detekci nového objektu pak navrhují využívat k nově zavedenou funkci, která využívá chyby rekonstrukce obou dekódovacích vrstev. Řada přístupů k ND s využitím autoenkodérů využívá různé modifikované struktury, ať již to jsou např. rekurentní autoenkodéry \cite{auto7,auto8}, variační autoenkodéry \cite{auto9} nebo tzv. self-adversarial variační autoenkodéry \cite{auto10}.


\section{Další vybrané metody využívající adaptivní filtry}
Vzhledem k tomu, že tématem předkladané dizertační práce jsou adaptivní systémy, byla tato kapitola vyčleněna metodám detekce novosti, které využívají právě adaptivní filtry. Zvláštní pozornost a podrobný popis se pak týká algoritmům Learning Entropy a Error and Learning Based Novelty Detection, které jsou v kapitole \ref{chap: ESE_vysledky} použity v porovnání výsledků.
\subsection{Metoda set-membership}
Metoda set-membership \cite{diniz0,set_member} je metoda použitá nejen ke zrychlení adaptace filtru ale i pro ND. Tato metoda pracuje s chybou predikce a adaptivním filtrem. Základní myšlenka je, že pokud je chyba predikce filtru $e$ větší než nějaká prahová hodnota $\gamma$, dojde k adaptaci tohoto filtru. Podmínka pro adaptaci filtru je tedy ve tvaru 
\begin{equation}
\abs{e(k)} > \gamma.
\end{equation} V okamžiku, kdy dochází k adaptaci filtru lze hovořit o  novosti v datech. V originálním článku \cite{set_member} autoři metodu využívají ke zrychlení konvergence NLMS algoritmu a přicházejí s novým způsobem adaptace vah filtru, algoritmem SM-NLMS. 
\par 
Diniz a Yazdanpanah v \cite{diniz1} používají metodu set-membership a navrhují nový algoritmus ST-SM-NLMS, který využívají k ořezávání datasetů. Autoři zde rozdělují data do dvou množin. V první množině jsou data přinášející nějakou informaci a ve druhé data které novou informaci nenesou. Základní myšlenka je, že pokud jsou pro adaptivní filtr data nová, bude chyba predikce v určitých mezích.  Tyto meze jsou dány dvěma prahovými hodnotami a pokud se chyba predikce filtru $e$ nachází v těchto mezích, přinášejí data z pohledu celého datasetu nějakou užitečnou informaci. Naopak data, pro která je chyba predikce adaptivního filtru malá tuto informaci nepřinášejí a stejně tak data, pro které je chyba predikce příliš velká jsou outlier a tedy nepřinášejí žádnou užitečnou informaci. Oproti klasickému SM-NLMS algoritmu, který pracuje s jedním prahem tedy autoři navrhují prahy dva. Podmínka pro adaptaci filtru je potom ve tvaru
\begin{equation}
\overline{\gamma}_1 \leq \abs{e(k)} \leq \overline{\gamma}_2
\end{equation}
Výsledná metoda slouží ke snížení výpočetního času pro adaptaci filtru a zároveň data klasifikuje do dvou výše zmíněných množin.
\par
Algoritmy založené na metodě set-membership nalezly široké uplatnění v řadě  oblastí. Jmenujme alespoň oblast mobilní robotiky, kde se používají např. při fúzi dat z vícero senzorů \cite{set_member_robot}, při lokalizaci a řízení podvodních robotů \cite{set_member_robot_voda} nebo při detekci poruch \cite{set_member_robot_chyba}. 
\subsection{Algoritmus Learning Entropy}\label{chap:LE}
Algoritmus Learning Entropy (LE) je algoritmem, který využívá adaptivních filtrů (obvykle LNU a HONU) k detekci novosti v datech \cite{ivoLE1,ivoLE2}. Autoři ukazují, že ne vždy, je novost v datech korelována s chybou predikce, a k detekci novosti navrhují využívat přírůstky vah adaptivních filtrů $\Delta\textbf{w}$, které jsou získány použitím gradientní metody a adaptací s každými novými daty. V první publikaci \cite{ivoLE1} je popsán multifraktální přístup, který využívá uspořádané množiny různých hodnot prahů
\begin{equation}
\boldsymbol{\alpha}=\{ \alpha_1,\alpha_1,\dots,\alpha_{n_{\alpha}} \}, \alpha_1>\alpha_2>\dots>\alpha_{n_{\alpha}}
\end{equation}
přičemž $\forall i \in \{1,2,\dots,n_{\alpha} \}:\alpha_i\in R$. Míra novosti "Approximate Individual Sample Learning Entropy" je potom definovaná jako
\begin{equation}\label{eq:aisle}
E_A(k)=\frac{1}{n\cdot n_{\alpha}}\sum_{i=1}^{n}\sum_j^{n_{\alpha}} f(\abs{\Delta w_i(k)}, \alpha_j)
\end{equation}
kde funkce $f$ je
\begin{equation}
f(\Delta w_i, \alpha_j)=\begin{cases}
1 \; if \; \abs{\Delta w_i(k)}>\alpha_{j} \cdot \overline{\abs{\Delta w_i^M(k)}}\\
0 \; if \; \abs{\Delta w_i(k)}\leq \alpha_j \cdot \overline{\abs{\Delta w_i^M(k)}}


\end{cases}
\end{equation}
kde $n$ je počet adaptivních parametrů filtru, $n_{\alpha}$ je počet citlivostních prahů (respektive počet prvků množiny $\boldsymbol{\alpha}$) a $\overline{\abs{\Delta w_i^M(k)}}$ průměrná hodnota přírůstku $i$-té adaptivní výhy v plovoucím okně délky $M$. Člen $1/(n\cdot n_{\alpha})$ normalizuje hodnotu AISLE takže  $E_A \in \langle 0,1\rangle$. Toto hodnotu lze interpretovat tak že pokud se blíží 0, váhy se téměř nemění a z pohledu adaptace filtru se příliš neliší od minulých dat. Naopak vysoké hodnoty $E_A$ znamenají, že dochází k velkým změnám velkého množství adaptivních vah a data se nějakým způsobem liší od předchozích. 
\par
V publikaci \cite{ivoLE2} pak autoři navrhují tzv. přímý algoritmus pro výpočet LE. Ta je vypočítána jako
\begin{equation}\label{eq:le_direct}
LE(k)=\sum_{i=1}^n \max \{0,z(\abs{\Delta w_i(k)})-\beta \};
\end{equation}
kde funkce $z$ je označovaná jako speciální $z$-score a je definovaná jako
\begin{equation}
z(\abs{\Delta w_i(k)}) = \frac{\abs{\Delta w_i(k)}-\overline{\abs{\Delta\textbf{w}_i^M(k-1)}}}{\sigma(\abs{\Delta \textbf{w}_i^M(k-1)})}
\end{equation}
kde
\begin{equation}
\overline{\abs{\Delta \textbf{w}_i^M(k-1)}}=\frac{\sum_{j=1}^M w_i(k-j-m)}{M}
\end{equation} 
je průměr hodnot přírůstku adaptivních vah, přičemž parametr $M$ je délka plovoucího okna, z kterého je spočítán průměr přírůstků $i$-té váhy a parametr $m$ je volitelný parametr pro data které vykazují periodicitu a $\sigma(\sigma(\abs{\Delta \textbf{w}_i^M(k-1)})$ je směrodatná odchylka těchto $M$ přírůstků. Pro takto definovanou LE pak platí, že $E(k)\in \langle0,+\infty)$. Tento přístup umožňuje detekovat změny adaptivních parametrů, které jsou větší než jejich průměr zvětšený o $\beta$ násobek jejich směrodatné odchylky a zároveň obchází nutnost výpočtu přes několik citlivostních prahů jako v případě \ref{eq:aisle}. Pokud bychom chtěli detekovat i neobvykle malé změny adaptivních parametrů, navrhují autoři vztah pro výpočet LE \ref{eq:le_direct} modifikovat na tvar
\begin{equation}\label{eq:le_direct_padasip}
E(k)=\sum_{i=1}^{n_w}z(\abs{\Delta w_i(k)}); E \in R
\end{equation}
který v případě negativních hodnot $z(\abs{\Delta w_i(k)})$ nemusí poskytovat jasnou hranici mezi obvyklými a neobvyklými změnami adaptivních parametrů.
\subsection{Algoritmus Error and Learning Based Novelty Detection}\label{chap:elbnd}
Algoritmus Error and Learning Based Novelty Detection (ELBND) \cite{elbnd1,elbnd2} je dalším z algoritmů detekce novosti, které využívají adaptivních filtrů, jejichž parametry jsou adaptovány vždy s novými daty. Každý naměřený vzorek dat je popsán vektorem hodnot, na základě přírůstku adaptivních vah a chyby filtru, definovaným jako
\begin{equation}
	elbnd(k)=\Delta\textbf{w}\cdot e(k)
\end{equation}
který popisuje novost v daném vzorku dat. Skalární hodnotu popisující novost ve vzorku dat pak autoři navrhují získat buď jako maximum z absolutních hodnot vektoru $elbnd(k)$, takže skalární míra novosti $ELBND(k)$ je definovaná jako
\begin{equation}\label{eq:elbnd1}
ELBND(k) = \max_{1\leq i \leq n_w} \abs{\Delta w_i(k) \cdot e(k)}.
\end{equation}
Další možností je určit míru novosti jako součet absolutních hodnot jednotlivých přírůstků násobených chybou filtru, tedy
\begin{equation}\label{eq:elbnd2}
	ELBND(k)=\sum_{i=1}^{n_w}\abs{\Delta w_i(k)\cdot e(k)}
\end{equation}

Uvedený algoritmus, využívající vztah \ref{eq:elbnd1}, byl úspěšně použit k adaptivní klasifikaci EEG pacientů s demencí \cite{elbnd3}.

