\chapter{Přehled metod detekce novosti}
Tato kapitola je věnována přehledu různých přístupů k detekci novosti, které se v posledních letech používají. Více pozornosti je věnováno algoritmům Learning Entropy (viz kap.\ref{chap:LE}) a Error and Learning Based Novelty detection (viz kap. \ref{chap:elbnd}) které jsou v kapitole (XYZ) použity pro porovnání úspěšnosti detekce novosti.
\par
Vzhledem k dlouhé historii oblasti detekce novosti vznikla celá řada přehledových publikací, které metody detekce novosti rozdělují do různých skupin. Miljkovič v \cite{miljkovic} rozděluje metody detekce novosti na metody založené na klasifikaci (pravidlové systémy, neuronové sítě a SVM), nejbližší sousedy (vzdálenost, hustota), klastrovací metody, statistické (parametrické a neparametrické) a ostatní (teorie informace, spektrální dekompozice, vizualizace).
\par 
Zimek a Filzmozer \cite{zimek} rozdělují metody ND opět na celou řadu kategorií. Na metody s učitelem a bez učitele, na metody založené na skóre nebo příslušnosti ke množině přičemž metody založené na skóre přiřazují datům nějaké číselné ohodnocení podle kterého se rozhoduje, zda jde o data nové či nikoliv. Metody založené na příslušnosti zařazují data rovnou do jedné ze dvou množin označené "normální" a "outlier". Další rozdělení je na metody parametrické a neparametrické. Mezi parametrické metody autoři řadí metody založené na konkrétních pravděpodobnostních rozložení (např. gaussovké). Neparametrické metody jsou metody, které nepracují s parametry pravděpodobnostních rozdělení. 
\par 
Jednu z nejobsáhlejších přehledových publikací napsali Markou a Singh \cite{markou1,markou2}. První díl jejich přehledu je věnován statistickým metodám ND a druhý díl je věnován metodám založených na neuronových sítích. V první části zabývající se statistickými metodami autoři přichazejí s dělením na metody parametrické (pravděpodobností, gaussian mixture modely, skryté markovské modely a testování hypotéz), a neparametrické (kNN, Parzenovo okno, porovnávání řetězců a klastrovací metody). Metody založené na neuronových sítích pak dělí ne přístupy využívající vícevrstvý perceptron, metodu podpůrných vektorů, teorii adaptivní rezonance, sítě typu RBF (radial basis function), autoasociátory, Hopfieldovy sítě, oscilující sítě, samoorganizujícíc se mapy, neuronové stromy, sítě s neasociativním učením a ostatní přístupy.
\par
Nejnovější extenzivní publikací je velice rozsáhlá přehledová publikace čítající přes 300 referencí byla vytvořena Pimentelem, D. a L. Cliftonovými a Tarassenkem \cite{pimentel}. Autoři zde dělí metody ND na pravděpodobnostní (parametrické a neparametrické), metody založené na vzdálenosti (nejbližší soused, klastrovací metody), metody založené na rekonstrukci (neuronové sítě, metody založené na podprostorech), metody vycházející z teorie informace a doménové metody (SVM).

\section{Metoda podpůrných vektorů}
Metody detekce novosti pomocí SVM vycházejí ze dvou různých přístupů. První přístup vychází předpokladu, že obvyklá data se vyskytují v oblastech s vysokou hustotou rozdělení. Data která se považují za nová potom leží v oblastech s nízkou hustotou.
\par 
Druhý přístup vyžaduje předem oklasifikovaná data .
\section{Metoda set-membership}
Metoda set-membership \cite{diniz0,set_member} je metoda použitá nejen ke zrychlení adaptace filtru ale i pro ND. Tato metoda pracuje s chybou predikce a adaptivním filtrem. Základní myšlenka je, že pokud je chyba predikce filtru $e$ větší než nějaká prahová hodnota $\gamma$, dojde k adaptaci tohoto filtru. Podmínka pro adaptaci filtru je tedy ve tvaru 
\begin{equation}
\abs{e(k)} > \gamma.
\end{equation} V okamžiku, kdy dochází k adaptaci filtru lze hovořit o  novosti v datech. V originálním článku \cite{set_member} autoři metodu využívají ke zrychlení konvergence NLMS algoritmu a přicházejí s novým způsobem adaptace vah filtru, algoritmem SM-NLMS. 
\par 
Diniz a Yazdanpanah v \cite{diniz1} používají metodu set-membership a navrhují nový algoritmus ST-SM-NLMS, který využívají k ořezávání datasetů. Autoři zde rozdělují data do dvou množin. V první množině jsou data přinášející nějakou informaci a ve druhé data které novou informaci nenesou. Základní myšlenka je, že pokud jsou pro adaptivní filtr data nová, bude chyba predikce v určitých mezích.  Tyto meze jsou dány dvěma prahovými hodnotami a pokud se chyba predikce filtru $e$ nachází v těchto mezích, přinášejí data z pohledu celého datasetu nějakou užitečnou informaci. Naopak data, pro která je chyba predikce adaptivního filtru malá tuto informaci nepřinášejí a stejně tak data, pro které je chyba predikce příliš velká jsou outlier a tedy nepřinášejí žádnou užitečnou informaci. Oproti klasickému SM-NLMS algoritmu, který pracuje s jedním prahem tedy autoři navrhují prahy dva. Podmínka pro adaptaci filtru je potom ve tvaru
\begin{equation}
\overline{\gamma}_1 \leq \abs{e(k)} \leq \overline{\gamma}_2
\end{equation}
Výsledná metoda slouží ke snížení výpočetního času pro adaptaci filtru a zároveň data klasifikuje do dvou výše zmíněných množin.
\par
Algoritmy založené na metodě set-membership nalezly široké uplatnění v řadě  oblastí. Jmenujme alespoň oblast mobilní robotiky, kde se používají např. při fúzi dat z vícero senzorů \cite{set_member_robot}, při lokalizaci a řízení podvodních robotů \cite{set_member_robot_voda} nebo při detekci poruch \cite{set_member_robot_chyba}. 
\section{Markovovy modely}

\section{Autoenkodéry}

\section{Neuronové sítě}
\subsection{Samoogranizující se mapy}

\subsection{Konvoluční neuronové sítě}


\section{Testování statistických hypotéz}

\section{Metody založené na teorii extrémních hodnot}


\section{Algoritmus Learning Entropy}\label{chap:LE}
Algoritmus Learning Entropy (LE) je algoritmem, který využívá adaptivních filtrů (obvykle LNU a HONU) k detekci novosti v datech \cite{ivoLE1,ivoLE2}. Autoři ukazují, že ne vždy, je novost v datech korelována s chybou predikce, a k detekci novosti navrhují využívat přírůstky vah adaptivních filtrů $\Delta\textbf{w}$, které jsou získány použitím gradientní metody a adaptací s každými novými daty. V první publikaci \cite{ivoLE1} je popsán multifraktální přístup, který využívá uspořádané množiny různých hodnot prahů
\begin{equation}
\boldsymbol{\alpha}=\{ \alpha_1,\alpha_1,\dots,\alpha_{n_{\alpha}} \}, \alpha_1>\alpha_2>\dots>\alpha_{n_{\alpha}}
\end{equation}
přičemž $\forall i \in \{1,2,\dots,n_{\alpha} \}:\alpha_i\in R$. Míra novosti "Approximate Individual Sample Learning Entropy" je potom definovaná jako
\begin{equation}\label{eq:aisle}
E_A(k)=\frac{1}{n\cdot n_{\alpha}}\sum_{i=1}^{n}\sum_j^{n_{\alpha}} f(\abs{\Delta w_i(k)}, \alpha_j)
\end{equation}
kde funkce $f$ je
\begin{equation}
f(\Delta w_i, \alpha_j)=\begin{cases}
1 \; if \; \abs{\Delta w_i(k)}>\alpha_{j} \cdot \overline{\abs{\Delta w_i^M(k)}}\\
0 \; if \; \abs{\Delta w_i(k)}\leq \alpha_j \cdot \overline{\abs{\Delta w_i^M(k)}}


\end{cases}
\end{equation}
kde $n$ je počet adaptivních parametrů filtru, $n_{\alpha}$ je počet citlivostních prahů (respektive počet prvků množiny $\boldsymbol{\alpha}$) a $\overline{\abs{\Delta w_i^M(k)}}$ průměrná hodnota přírůstku $i$-té adaptivní výhy v plovoucím okně délky $M$. Člen $1/(n\cdot n_{\alpha})$ normalizuje hodnotu AISLE takže  $E_A \in \langle 0,1\rangle$. Toto hodnotu lze interpretovat tak že pokud se blíží 0, váhy se téměř nemění a z pohledu adaptace filtru se příliš neliší od minulých dat. Naopak vysoké hodnoty $E_A$ znamenají, že dochází k velkým změnám velkého množství adaptivních vah a data se nějakým způsobem liší od předchozích. 
\par
V publikaci \cite{ivoLE2} pak autoři navrhují tzv. přímý algoritmus pro výpočet LE. Ta je vypočítána jako
\begin{equation}\label{eq:le_direct}
LE(k)=\sum_{i=1}^n \max \{0,z(\abs{\Delta w_i(k)})-\beta \};
\end{equation}
kde funkce $z$ je označovaná jako speciální $z$-score a je definovaná jako
\begin{equation}
z(\abs{\Delta w_i(k)}) = \frac{\abs{\Delta w_i(k)}-\overline{\abs{\Delta\textbf{w}_i^M(k-1)}}}{\sigma(\abs{\Delta \textbf{w}_i^M(k-1)})}
\end{equation}
kde
\begin{equation}
\overline{\abs{\Delta \textbf{w}_i^M(k-1)}}=\frac{\sum_{j=1}^M w_i(k-j-m)}{M}
\end{equation} 
je průměr hodnot přírůstku adaptivních vah, přičemž parametr $M$ je délka plovoucího okna, z kterého je spočítán průměr přírůstků $i$-té váhy a parametr $m$ je volitelný parametr pro data které vykazují periodicitu a $\sigma(\sigma(\abs{\Delta \textbf{w}_i^M(k-1)})$ je směrodatná odchylka těchto $M$ přírůstků. Pro takto definovanou LE pak platí, že $E(k)\in \langle0,+\infty)$. Tento přístup umožňuje detekovat změny adaptivních parametrů, které jsou větší než jejich průměr zvětšený o $\beta$ násobek jejich směrodatné odchylky a zároveň obchází nutnost výpočtu přes několik citlivostních prahů jako v případě \ref{eq:aisle}. Pokud bychom chtěli detekovat i neobvykle malé změny adaptivních parametrů, navrhují autoři vztah pro výpočet LE \ref{eq:le_direct} modifikovat na tvar
\begin{equation}\label{eq:le_direct_padasip}
E(k)=\sum_{i=1}^{n_w}z(\abs{\Delta w_i(k)}); E \in R
\end{equation}
který v případě negativních hodnot $z(\abs{\Delta w_i(k)})$ nemusí poskytovat jasnou hranici mezi obvyklými a neobvyklými změnami adaptivních parametrů.
\section{Algoritmus Error and Learning Based Novelty Detection}\label{chap:elbnd}
Algoritmus Error and Learning Based Novelty Detection (ELBND) \cite{elbnd1,elbnd2} je dalším z algoritmů detekce novosti, které využívají adaptivních filtrů, jejichž parametry jsou adaptovány vždy s novými daty. Každý naměřený vzorek dat je popsán vektorem hodnot, na základě přírůstku adaptivních vah a chyby filtru, definovaným jako
\begin{equation}
	elbnd(k)=\Delta\textbf{w}\cdot e(k)
\end{equation}
který popisuje novost v daném vzorku dat. Skalární hodnotu popisující novost ve vzorku dat pak autoři navrhují získat buď jako maximum z absolutních hodnot vektoru $elbnd(k)$, takže skalární míra novosti $ELBND(k)$ je definovaná jako
\begin{equation}\label{eq:elbnd1}
ELBND(k) = \max_{1\leq i \leq n_w} \abs{\Delta w_i(k) \cdot e(k)}.
\end{equation}
Další možností je určit míru novosti jako součet absolutních hodnot jednotlivých přírůstků násobených chybou filtru, tedy
\begin{equation}\label{eq:elbnd2}
	ELBND(k)=\sum_{i=1}^{n_w}\abs{\Delta w_i(k)\cdot e(k)}
\end{equation}

Uvedený algoritmus, využívající vztah \ref{eq:elbnd1}, byl úspěšně použit k adaptivní klasifikaci EEG pacientů s demencí \cite{elbnd3}.

\section{Aplikace metod detekce novosti}
